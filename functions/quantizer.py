# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10D7ImfUcfNdv3rOhBIaV_zpcVTIxE42o
"""

import torch
import torch.nn as nn
import math
import numpy as np # Used only for printing clarity in verification

# ============================================================================
# PART A: ORIGINAL UNIFORMQUANTIZER (For Direct Comparison)
# ============================================================================

class UniformQuantizer(nn.Module):
    def __init__(self, n_bits=8, symmetric=True, channel_wise=False, momentum=0.1, ch_axis=0, eps=1e-8):
        super().__init__()
        self.n_bits = int(n_bits)
        self.symmetric = bool(symmetric)
        self.channel_wise = bool(channel_wise)
        self.momentum = float(momentum)
        self.ch_axis = int(ch_axis)
        self.eps = float(eps)

        if self.symmetric:
            self.qmin = -(2 ** (self.n_bits - 1))
            self.qmax = 2 ** (self.n_bits - 1) - 1
        else:
            self.qmin = 0
            self.qmax = 2 ** self.n_bits - 1

        # safe neutral init
        self.register_buffer("scale", torch.tensor(1.0, dtype=torch.float32))
        self.register_buffer("zero_point", torch.tensor(0.0, dtype=torch.float32))
        #self.register_buffer("scale", torch.tensor(1.0, dtype=torch.float32), persistent=False) #to prevent it from appearing in the keys
        #self.register_buffer("zero_point", torch.tensor(0.0, dtype=torch.float32), persistent=False) #to prevent it from appearing in the keys
        self._calibrated = False

    def _ensure_shape(self, sample_scale):
        # ensure self.scale has the same shape as sample_scale
        if self.scale.shape != sample_scale.shape:
            with torch.no_grad():
                s = torch.zeros_like(sample_scale, device=self.scale.device, dtype=self.scale.dtype)
                z = torch.zeros_like(sample_scale, device=self.zero_point.device, dtype=self.zero_point.dtype)
                # replace buffers in-place (so state_dict keeps names)
                self.scale = s
                self.zero_point = z
            self._calibrated = True

    def calculate_qparams(self, x):
        if self.channel_wise:
            dims = [i for i in range(x.ndim) if i != self.ch_axis]
            x_min = x.amin(dim=dims, keepdim=True)
            x_max = x.amax(dim=dims, keepdim=True)
        else:
            x_min = x.min()
            x_max = x.max()

        if self.symmetric:
            x_absmax = torch.maximum(x_min.abs(), x_max.abs())
            qmax_sym = float(2 ** (self.n_bits - 1) - 1)
            scale = (x_absmax / qmax_sym).clamp(min=self.eps)
            zero_point = torch.zeros_like(scale)
        else:
            denom = float(self.qmax - self.qmin)
            scale = ((x_max - x_min) / denom).clamp(min=self.eps)
            zero_point = (self.qmin - torch.round(x_min / scale)).clamp(min=self.qmin, max=self.qmax)

        return scale, zero_point

    @torch.no_grad()
    def init_from_tensor(self, x):
        """Initialize scale/zero_point from a sample tensor (e.g. weights)."""
        scale, zp = self.calculate_qparams(x)
        self._ensure_shape(scale)
        # copy into buffers in-place
        self.scale.copy_(scale.detach().to(self.scale.device))
        self.zero_point.copy_(zp.detach().to(self.zero_point.device))
        self._calibrated = True

    def forward(self, x, simulate_int=False):
        # training: fake quantize; eval: dequantized floats by default
        if self.training:
            scale_b, zp_b = self.calculate_qparams(x)
            if self.channel_wise:
                self._ensure_shape(scale_b)
            # EMA update (in-place)
            with torch.no_grad():
                s_new = scale_b.detach().to(self.scale.device)
                z_new = zp_b.detach().to(self.zero_point.device)
                # in-place update to preserve buffer objects in state_dict
                if self.scale.shape == s_new.shape:
                    self.scale.mul_(1.0 - self.momentum).add_(self.momentum * s_new)
                    self.zero_point.mul_(1.0 - self.momentum).add_(self.momentum * z_new)
                else:
                    # broadcast-safe assignment
                    self.scale.copy_((1.0 - self.momentum) * self.scale + self.momentum * s_new)
                    self.zero_point.copy_((1.0 - self.momentum) * self.zero_point + self.momentum * z_new)

            # Fake quantize (STE-like)
            x_int = torch.round(x / self.scale + self.zero_point)
            x_int = torch.clamp(x_int, self.qmin, self.qmax)
            x_dequant = self.scale * (x_int - self.zero_point)
            # final clamp to avoid inf/nan (extra safety)
            x_dequant = torch.nan_to_num(x_dequant, nan=0.0, posinf=1e6, neginf=-1e6)
            return x_dequant

        else:
            # eval: default -> dequantized floats (safe)
            x_int = torch.round(x / self.scale + self.zero_point)
            x_int = torch.clamp(x_int, self.qmin, self.qmax)
            x_dequant = self.scale * (x_int - self.zero_point)
            return x_dequant if not simulate_int else x_int.to(torch.int8)

# ============================================================================
# PART B: MANUAL PYTORCH QUANTIZER (Uses only low-level tensor ops)
# FIXED: Added _ensure_shape and persistence to buffers.
# ============================================================================

class ManualUniformQuantizer(nn.Module):
    """
    Fully Manual Uniform Quantizer with Explicit Low-Level Operations.

    All high-level PyTorch APIs are replaced with manual implementations
    to provide full control over the quantization dataflow for hardware mapping.
    """

    def __init__(self, n_bits=8, symmetric=True, channel_wise=False, ch_axis=0, eps=1e-8):
        """
        Args:
            n_bits: Number of quantization bits
            symmetric: Symmetric vs asymmetric quantization
            channel_wise: Per-channel or global quantization
            ch_axis: Channel axis for per-channel quantization
            eps: Small epsilon to prevent division by zero
        """
        super().__init__()
        self.n_bits = int(n_bits)
        self.symmetric = bool(symmetric)
        self.channel_wise = bool(channel_wise)
        self.ch_axis = int(ch_axis)
        self.eps = float(eps)

        if self.symmetric:
            self.qmin = -(2 ** (self.n_bits - 1))
            self.qmax = 2 ** (self.n_bits - 1) - 1
        else:
            self.qmin = 0
            self.qmax = 2 ** self.n_bits - 1

        # Buffers for scale and zero_point
        self.register_buffer("scale", torch.tensor(1.0, dtype=torch.float32), persistent=False)
        self.register_buffer("zero_point", torch.tensor(0.0, dtype=torch.float32), persistent=False)
        self._calibrated = False

    # ========================================================================
    # MODIFIED: Simple Manual Rounding (Round Half Away From Zero)
    # ========================================================================

    def manual_round(self, x):
        """
        Simple rounding: round half away from zero.
        2.5 -> 3, -2.5 -> -3

        Implementation:
        - Get sign of x
        - Take absolute value
        - Add 0.5 and floor
        - Restore sign
        """
        sign = torch.where(x < 0, torch.tensor(-1.0, dtype=x.dtype, device=x.device),
                          torch.tensor(1.0, dtype=x.dtype, device=x.device))
        abs_x = torch.where(x < 0, -x, x)  # Manual abs
        rounded = torch.floor(abs_x + 0.5)
        return sign * rounded

    # ========================================================================
    # MODIFIED: Manual Absolute Value and Element-wise Max
    # ========================================================================

    def manual_abs(self, x):
        """
        Manual absolute value: |x|
        Replaces torch.abs() with explicit sign check.
        """
        return torch.where(x < 0, -x, x)

    def manual_element_max(self, a, b):
        """
        Element-wise maximum between two tensors.
        Replaces torch.max(a, b) with explicit comparison.
        """
        return torch.where(a > b, a, b)

    # ========================================================================
    # Shape Management (keeping from original)
    # ========================================================================

    def _ensure_shape(self, sample_scale):
        """
        Ensures scale/zero_point buffers match the shape of the calculated qparams.
        """
        if self.scale.shape != sample_scale.shape:
            with torch.no_grad():
                s = torch.zeros_like(sample_scale, device=self.scale.device, dtype=self.scale.dtype)
                z = torch.zeros_like(sample_scale, device=self.zero_point.device, dtype=self.zero_point.dtype)

                # Reassign the buffers to the new shape
                self.scale = s
                self.zero_point = z
            self._calibrated = True

    # ========================================================================
    # MODIFIED: Calculate Quantization Parameters with Manual Operations
    # ========================================================================

    @torch.no_grad()
    def calculate_qparams(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Calculate scale and zero_point using manual operations where critical.
        """
        if self.channel_wise:
            dims = tuple(i for i in range(x.ndim) if i != self.ch_axis)
            x_min = x.amin(dim=dims, keepdim=True)
            x_max = x.amax(dim=dims, keepdim=True)
        else:
            # Global min/max
            x_min = x.min()
            x_max = x.max()

        if self.symmetric:
            # MODIFIED: Use manual_abs and manual_element_max
            x_absmax = self.manual_element_max(self.manual_abs(x_min), self.manual_abs(x_max))
            qmax_sym = float(2 ** (self.n_bits - 1) - 1)

            # scale = x_absmax / qmax_sym, then clamp
            scale = (x_absmax / qmax_sym).clamp(min=self.eps)

            zero_point = torch.zeros_like(scale)
        else:
            denom = float(self.qmax - self.qmin)

            # scale = (x_max - x_min) / denom, then clamp
            scale = ((x_max - x_min) / denom).clamp(min=self.eps)

            # zero_point = qmin - round(x_min / scale)
            # MODIFIED: Use manual_round instead of torch.round
            zero_point = (self.qmin - self.manual_round(x_min / scale)).clamp(min=self.qmin, max=self.qmax)

        return scale.to(torch.float32), zero_point.to(torch.float32)

    # ========================================================================
    # Initialization from Tensor
    # ========================================================================

    @torch.no_grad()
    def init_from_tensor(self, x: torch.Tensor):
        """
        Initialize scale/zero_point from a sample tensor (e.g., weights or activations).
        """
        scale, zp = self.calculate_qparams(x)
        self._ensure_shape(scale)  # Ensure shape before copy

        self.scale.copy_(scale)
        self.zero_point.copy_(zp)

        self._calibrated = True

    # ========================================================================
    # MODIFIED: Forward Pass with Manual Operations
    # ========================================================================

    def forward(self, x: torch.Tensor, simulate_int=False) -> torch.Tensor:
        """
        Forward pass: Quantize and dequantize using manual operations.

        MODIFIED Operations:
        1. Rounding uses manual implementation (round half away from zero)
        2. All critical operations are explicit and controllable
        """
        if not self._calibrated:
            self.init_from_tensor(x)

        # ====================================================================
        # QUANTIZATION: x_float -> x_int
        # ====================================================================

        # Divide by scale
        x_scaled = x / self.scale

        # Add zero_point
        x_shifted = x_scaled + self.zero_point

        # MODIFIED: Use manual_round instead of torch.round
        x_rounded = self.manual_round(x_shifted)

        # Clamp to quantization range
        x_int = x_rounded.clamp(self.qmin, self.qmax)

        # If simulate_int mode, return quantized integers
        if simulate_int:
            return x_int.to(torch.int8)

        # ====================================================================
        # DEQUANTIZATION: x_int -> x_float
        # ====================================================================

        # Subtract zero_point
        x_centered = x_int - self.zero_point

        # Multiply by scale (dequantization)
        x_dequant = self.scale * x_centered

        # Safety: Replace NaN/Inf with safe values
        x_dequant = torch.nan_to_num(x_dequant, nan=0.0, posinf=1e6, neginf=-1e6)

        return x_dequant.to(torch.float32)

    def __call__(self, x: torch.Tensor, simulate_int=False) -> torch.Tensor:
        return self.forward(x, simulate_int)

import torch
import numpy as np

def verify_quantizer_pytorch_extended(
    n_bits_list=[4, 6, 8, 12],
    symmetric_modes=[True, False],
    channel_wise_modes=[False, True],
    ch_axis=2,
    tolerance=1e-5,
):
    """Extended verification suite for Manual vs Original quantizers."""

    torch.manual_seed(42)

    # Synthetic data simulating real activation range
    B, H, C = 5, 4, 128
    torch_input = torch.randn(B, H, C) * 10.0 + 5.0

    print("=" * 80)
    print("🌟 EXTENDED QUANTIZER VERIFICATION")
    print("=" * 80)
    print(f"Input Tensor Shape: {list(torch_input.shape)} | Range: [{torch_input.min():.2f}, {torch_input.max():.2f}]")
    print("-" * 80)

    for n_bits in n_bits_list:
        for symmetric in symmetric_modes:
            for channel_wise in channel_wise_modes:
                print(f"\n🧩 TEST CONFIG → {n_bits}-bit | "
                      f"{'Symmetric' if symmetric else 'Asymmetric'} | "
                      f"{'Channel-wise' if channel_wise else 'Global'}")

                # Initialize quantizers
                manual_q = ManualUniformQuantizer(
                    n_bits=n_bits, symmetric=symmetric,
                    channel_wise=channel_wise, ch_axis=ch_axis
                ).eval()

                original_q = UniformQuantizer(
                    n_bits=n_bits, symmetric=symmetric,
                    channel_wise=channel_wise, ch_axis=ch_axis
                ).eval()

                # Calibrate both on the same tensor
                manual_q.init_from_tensor(torch_input)
                original_q.init_from_tensor(torch_input)

                # Forward pass (simulate dequantized floats)
                with torch.no_grad():
                    manual_out = manual_q(torch_input)
                    orig_out = original_q(torch_input)
                    manual_int = manual_q(torch_input, simulate_int=True)
                    orig_int = original_q(torch_input, simulate_int=True)

                # === Compare Outputs ===
                abs_diff = (manual_out - orig_out).abs()
                max_diff = abs_diff.max().item()
                mean_diff = abs_diff.mean().item()

                # Quantization noise metrics
                noise = manual_out - torch_input
                signal_power = torch.mean(torch_input ** 2).item()
                noise_power = torch.mean(noise ** 2).item()
                snr_db = 10 * np.log10(signal_power / noise_power + 1e-12)

                print(f"→ Max Diff (Dequant): {max_diff:.6e}")
                print(f"→ Mean Diff (Dequant): {mean_diff:.6e}")
                print(f"→ Quantization SNR: {snr_db:.2f} dB")

                # === Show example slices ===
                input_slice = torch_input.flatten()[:5].numpy()
                manual_int_slice = manual_int.flatten()[:5].numpy()
                orig_int_slice = orig_int.flatten()[:5].numpy()
                manual_deq_slice = manual_out.flatten()[:5].numpy()
                orig_deq_slice = orig_out.flatten()[:5].numpy()

                print("\n[Example Elements]")
                print("Input Floats        :", np.round(input_slice, 4))
                print("Manual Quant (int)  :", manual_int_slice)
                print("Original Quant (int):", orig_int_slice)
                print("Manual Dequant      :", np.round(manual_deq_slice, 4))
                print("Original Dequant    :", np.round(orig_deq_slice, 4))

                # === Internal scale check ===
                scale_mean = manual_q.scale.mean().item()
                zp_mean = manual_q.zero_point.mean().item()
                print(f"\nScale Mean: {scale_mean:.5f} | Zero Point Mean: {zp_mean:.5f}")

                # === Pass/Fail ===
                if max_diff < tolerance:
                    print("✅ RESULT: PASS (Manual == Original),")
                else:
                    print("❌ RESULT: FAIL (Difference Exceeds Tolerance)")

                print("-" * 80)


# ===========================================================
# Run the extended test
# ===========================================================
if __name__ == "__main__":
    verify_quantizer_pytorch_extended()