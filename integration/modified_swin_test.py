# -*- coding: utf-8 -*-
"""modified_swin_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AiMqpR87hlfqzBmajKiDtkwGIP8jFrwg
"""

import numpy as np
import torch
import torch.nn as nn
import math
from timm.models.layers import to_2tuple
import torch.utils.checkpoint as checkpoint
from timm.models.layers import DropPath, to_2tuple, trunc_normal_
from typing import Union, Tuple
from adaptive_avg_pool import AdaptiveAvgPool1d
from LayerNorm_v4 import LayerNorm
from linspace import linspace_list
from ModuleList import ModuleList
from linear import ExplicitLinear
from softmax import Softmax
from conv2d import MyConv2d as ManualConv2d
from GELU import GELU

WindowProcess = None
WindowProcessReverse = None
print("[Warning] Fused window process have not been installed.")

class ManualMlp:
    """
    Manual, NumPy-based implementation of the Swin Transformer MLP block (Feed-Forward Network).
    This block performs the expansion, activation, and contraction steps.
    """
    def __init__(self, W1, B1, W2, B2, in_features=None, hidden_features=None, out_features=None, act_layer=GELU, drop=0.):
        # Extract dimensions from weight shapes
        in_features = W1.shape[1]
        hidden_features = W1.shape[0]
        out_features = W2.shape[0]
        
        # FC1: expansion layer with provided weights
        self.fc1 = ExplicitLinear(
            in_features=in_features,
            out_features=hidden_features,
            weight=W1,
            bias=B1
        )
        
        # Activation
        self.act = act_layer()
        
        # FC2: contraction layer with provided weights
        self.fc2 = ExplicitLinear(
            in_features=hidden_features,
            out_features=out_features,
            weight=W2,
            bias=B2
        )

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)
        return x
    
    def __call__(self, x):
        """Make the class callable"""
        return self.forward(x)

    def window_partition(x, window_size):
        """Partition into non-overlapping windows - Pure NumPy"""
        # Convert to numpy if it's a tensor
        if isinstance(x, torch.Tensor):
            x = x.detach().cpu().numpy()

        B, H, W, C = x.shape
        x = x.reshape(B, H // window_size, window_size, W // window_size, window_size, C)
        # NumPy transpose accepts tuple of axes
        windows = x.transpose(0, 1, 3, 2, 4, 5).reshape(-1, window_size, window_size, C)
        return windows

    def window_reverse(windows, window_size, H, W):
        """Reverse window partition - Pure NumPy"""
        # Convert to numpy if it's a tensor
        if isinstance(windows, torch.Tensor):
            windows = windows.detach().cpu().numpy()

        B = int(windows.shape[0] / (H * W / window_size / window_size))
        x = windows.reshape(B, H // window_size, W // window_size, window_size, window_size, -1)
        # NumPy transpose accepts tuple of axes
        x = x.transpose(0, 1, 3, 2, 4, 5).reshape(B, H, W, -1)
        return x


def window_partition(x, window_size):
    """Partition into non-overlapping windows"""
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """Reverse window partition"""
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class WindowAttention(nn.Module):
    r""" Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qkv_bias (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, 
             qkv_weight=None, qkv_bias=None,
             proj_weight=None, proj_bias=None,
             qkv_bias_enabled=True, qk_scale=None, 
             attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer("relative_position_index", relative_position_index)

        # Replace nn.Linear with ExplicitLinear
        self.qkv = ExplicitLinear(in_features=dim, out_features= dim * 3,
                                          weight=qkv_weight, bias=qkv_bias, bias_condition=qkv_bias_enabled)
        
        self.attn_drop = nn.Dropout(attn_drop)

        self.proj = ExplicitLinear(in_features=dim, out_features= dim,
                                          weight=proj_weight, bias=proj_bias, bias_condition=True)
        
        self.proj_drop = nn.Dropout(proj_drop)


        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = Softmax(dim=-1)

    def forward(self, x, mask=None):
        """
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """
        B_, N, C = x.shape

        if isinstance(x, np.ndarray):
            x = torch.from_numpy(x).float()
        
        # Convert to numpy for ExplicitLinear, then back to torch
        x_np = x.detach().cpu().numpy()
        qkv_np = self.qkv(x_np)
        qkv = torch.from_numpy(qkv_np).to(x.device).float()
        
        qkv = qkv.reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))

        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        if isinstance(attn, np.ndarray):
            attn = torch.from_numpy(attn).to(x.device)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        
        # Convert to numpy for ExplicitLinear, then back to torch
        x_np = x.detach().cpu().numpy()
        x_proj_np = self.proj(x_np)
        x = torch.from_numpy(x_proj_np).to(x.device).float()
        
        x = self.proj_drop(x)
        return x

    def extra_repr(self) -> str:
        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'

    def flops(self, N):
        # calculate flops for 1 window with token length of N
        flops = 0
        # qkv = self.qkv(x)
        flops += N * self.dim * 3 * self.dim
        # attn = (q @ k.transpose(-2, -1))
        flops += self.num_heads * N * (self.dim // self.num_heads) * N
        #  x = (attn @ v)
        flops += self.num_heads * N * N * (self.dim // self.num_heads)
        # x = self.proj(x)
        flops += N * self.dim * self.dim
        return flops


class ManualSwinTransformerBlock:
    """
    Manual NumPy-based implementation of Swin Transformer Block
    Accepts extracted weights from PyTorch version
    """
    def __init__(self, 
                 # Architecture parameters
                 dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4.,
                 # Extracted weights
                 norm1_weight=None, norm1_bias=None,
                 attn_weights=None,  # Dictionary with attention weights
                 norm2_weight=None, norm2_bias=None,
                 mlp_fc1_weight=None, mlp_fc1_bias=None,
                 mlp_fc2_weight=None, mlp_fc2_bias=None,
                 attn_mask=None, qkv_bias_enabled=True, fused_window_process=False):
        """
        Args:
            dim: Number of input channels
            input_resolution: Input resolution tuple (H, W)
            num_heads: Number of attention heads
            window_size: Window size
            shift_size: Shift size for SW-MSA
            mlp_ratio: MLP hidden dimension ratio
            
            Weights extracted from PyTorch:
            norm1_weight, norm1_bias: LayerNorm 1 parameters
            attn_weights: Dictionary containing all attention module weights
            norm2_weight, norm2_bias: LayerNorm 2 parameters
            mlp_fc1_weight, mlp_fc1_bias: MLP first layer weights
            mlp_fc2_weight, mlp_fc2_bias: MLP second layer weights
            attn_mask: Attention mask for shifted windows
        """
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        self.window_partition = ManualMlp.window_partition
        self.window_reverse = ManualMlp.window_reverse
        
        # Adjust window size and shift size if needed
        if min(self.input_resolution) <= self.window_size:
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        
        # LayerNorm 1
        self.norm1 = LayerNorm(dim, weight=norm1_weight, bias=norm1_bias, bias_condition=True)
        
        # Window Attention (you'll need to implement this to accept weights)
        self.attn = WindowAttention(
            dim=dim,
            window_size=to_2tuple(self.window_size),
            num_heads=num_heads,
            qkv_weight=attn_weights['qkv_weight'],  # Pass all attention weights
            qkv_bias=attn_weights['qkv_bias'],  # Pass all attention weights
            proj_weight=attn_weights['proj_weight'],  # Pass all attention weights
            proj_bias=attn_weights['proj_bias'],  # Pass all attention weights
            qkv_bias_enabled=qkv_bias_enabled  # Pass all attention bias
        )
        
        # LayerNorm 2
        self.norm2 = LayerNorm(dim, weight=norm2_weight, bias=norm2_bias, bias_condition=True)
        
        # MLP
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = ManualMlp(mlp_fc1_weight, mlp_fc1_bias, mlp_fc2_weight, mlp_fc2_bias,
                             in_features=dim, hidden_features=mlp_hidden_dim, act_layer=GELU)
    
        # In SwinTransformerBlock.py, ManualSwinTransformerBlock.__init__()

        if self.shift_size > 0:
            # Calculate attention mask for SW-MSA using NumPy
            H, W = self.input_resolution
            img_mask = np.zeros((1, H, W, 1))  # Use numpy instead of torch.zeros

            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = self.window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.reshape(-1, self.window_size * self.window_size)  # NumPy uses reshape, not view

            # Use NumPy broadcasting instead of unsqueeze
            attn_mask = mask_windows[:, np.newaxis, :] - mask_windows[:, :, np.newaxis]

            # Use np.where instead of masked_fill
            attn_mask = np.where(attn_mask != 0, -100.0, 0.0).astype(np.float32)

            # Convert to torch tensor for compatibility with attention operations
            attn_mask = torch.from_numpy(attn_mask)
        else:
            attn_mask = None

        self.attn_mask = attn_mask

        self.fused_window_process = fused_window_process

    def forward(self, x):
        """
        Forward pass that handles both PyTorch tensors and NumPy arrays
        """
        # Check input type and convert if necessary
        is_tensor_input = isinstance(x, torch.Tensor)
        if is_tensor_input:
            # Convert to numpy for processing
            device = x.device
            dtype = x.dtype
            x = x.detach().cpu().numpy()
        
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"
    
        shortcut = x
        x = self.norm1(x)  # LayerNorm should handle both numpy and tensor
        
        # Ensure we have numpy array after norm
        if isinstance(x, torch.Tensor):
            x = x.detach().cpu().numpy()
        
        x = x.reshape(B, H, W, C)
    
        # cyclic shift - now x is definitely numpy
        if self.shift_size > 0:
            shifted_x = np.roll(x, shift=(-self.shift_size, -self.shift_size), axis=(1, 2))
            x_windows = self.window_partition(shifted_x, self.window_size)
        else:
            shifted_x = x
            x_windows = self.window_partition(shifted_x, self.window_size)
    
        x_windows = x_windows.reshape(-1, self.window_size * self.window_size, C)
    
        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=self.attn_mask)
    
        # merge windows
        attn_windows = attn_windows.reshape(-1, self.window_size, self.window_size, C)
    
        # reverse cyclic shift
        if self.shift_size > 0:
            shifted_x = self.window_reverse(attn_windows, self.window_size, H, W)
            x = np.roll(shifted_x, shift=(self.shift_size, self.shift_size), axis=(1, 2))
        else:
            shifted_x = self.window_reverse(attn_windows, self.window_size, H, W)
            x = shifted_x
    
        x = x.reshape(B, H * W, C)
        x = shortcut + x
    
        # FFN
        x = x + self.mlp(self.norm2(x))
    
        # Convert back to tensor if input was tensor
        if is_tensor_input:
            x = torch.from_numpy(x).to(device=device, dtype=dtype)
    
        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, " \
               f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

    def flops(self):
        flops = 0
        H, W = self.input_resolution
        # norm1
        flops += self.dim * H * W
        # W-MSA/SW-MSA
        nW = H * W / self.window_size / self.window_size
        flops += nW * self.attn.flops(self.window_size * self.window_size)
        # mlp
        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
        # norm2
        flops += self.dim * H * W
        return flops


class ManualPatchMerging:
    r""" Patch Merging Layer.

    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, input_resolution, dim, reduction_weight=None, norm_weight=None, norm_bias=None, norm_layer=LayerNorm):
        self.input_resolution = input_resolution
        self.dim = dim

        self.reduction = ExplicitLinear(
            in_features=4 * dim, 
            out_features=2 * dim,
            weight=reduction_weight, 
            bias_condition=False
        )
        
        self.norm = norm_layer(
            normalized_shape=4 * dim, 
            weight=norm_weight, 
            bias=norm_bias
        )

    def forward(self, x):
        """
        x: B, H*W, C
        """
        # Convert to numpy if it's a tensor
        is_tensor = isinstance(x, torch.Tensor)
        if is_tensor:
            x_np = x.detach().cpu().numpy()
        else:
            x_np = x
        
        H, W = self.input_resolution
        B, L, C = x_np.shape
        assert L == H * W, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

        x_np = x_np.reshape(B, H, W, C)

        x0 = x_np[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x_np[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x_np[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x_np[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x_np = np.concatenate([x0, x1, x2, x3], axis=-1)  # B H/2 W/2 4*C
        x_np = x_np.reshape(B, -1, 4 * C)  # B H/2*W/2 4*C

        x_np = self.norm(x_np)
        x_np = self.reduction(x_np)

        return x_np

    def extra_repr(self) -> str:
        return f"input_resolution={self.input_resolution}, dim={self.dim}"

    def flops(self):
        H, W = self.input_resolution
        flops = H * W * self.dim
        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim
        return flops


class ManualBasicLayer:
    """A basic Swin Transformer layer for one stage - Fixed Version
    
    This version accepts per-block weights to match PyTorch's independent block weights.
    """

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., blocks_weights=None, downsample_weights=None,
                 norm_layer=None, downsample=None, use_checkpoint=False,
                 fused_window_process=False, qkv_bias=True):
        """
        Args:
            dim: Number of input channels
            input_resolution: Input resolution tuple (H, W)
            depth: Number of blocks
            num_heads: Number of attention heads
            window_size: Local window size
            mlp_ratio: Ratio of mlp hidden dim to embedding dim
            blocks_weights: List of weight dictionaries, one per block
            downsample_weights: Dictionary of downsample layer weights
            norm_layer: Normalization layer class
            downsample: Downsample layer class
            use_checkpoint: Whether to use checkpointing
            fused_window_process: Whether to use fused window processing
        """
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # Build blocks with individual weights
        self.blocks = []
        if blocks_weights is None:
            raise ValueError("blocks_weights must be provided")
        
        if len(blocks_weights) != depth:
            raise ValueError(f"Expected {depth} block weights, got {len(blocks_weights)}")
        
        for i, block_w in enumerate(blocks_weights):
            # Import ManualSwinTransformerBlock from your module
            from SwinTransformerBlock import ManualSwinTransformerBlock
            
            block = ManualSwinTransformerBlock(
                dim=dim,
                input_resolution=input_resolution,
                num_heads=num_heads,
                window_size=window_size,
                shift_size=block_w['shift_size'],
                mlp_ratio=mlp_ratio,
                norm1_weight=block_w['norm1_weight'],
                norm1_bias=block_w['norm1_bias'],
                attn_weights=block_w['attn_weights'],
                norm2_weight=block_w['norm2_weight'],
                norm2_bias=block_w['norm2_bias'],
                mlp_fc1_weight=block_w['mlp_fc1_weight'],
                mlp_fc1_bias=block_w['mlp_fc1_bias'],
                mlp_fc2_weight=block_w['mlp_fc2_weight'],
                mlp_fc2_bias=block_w['mlp_fc2_bias'],
                attn_mask=block_w['attn_mask'],
                qkv_bias_enabled=qkv_bias,
                fused_window_process=fused_window_process
            )
            self.blocks.append(block)

        # Patch merging layer
        if downsample is not None:
            if downsample_weights is None:
                raise ValueError("downsample_weights must be provided when downsample is not None")
            
            self.downsample = downsample(
                input_resolution=input_resolution,
                dim=dim,
                norm_layer=norm_layer,
                reduction_weight=downsample_weights['reduction_weight'],
                norm_weight=downsample_weights['norm_weight'],
                norm_bias=downsample_weights['norm_bias']
            )
        else:
            self.downsample = None

    def forward(self, x):
        """Forward pass through all blocks and optional downsample"""
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk.forward, x)
            else:
                x = blk.forward(x)
        if self.downsample is not None:
            x = self.downsample.forward(x)
        return x
    
    def __call__(self, x):
        """Make the layer callable"""
        return self.forward(x)

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"

    def flops(self):
        flops = 0
        for blk in self.blocks:
            flops += blk.flops()
        if self.downsample is not None:
            flops += self.downsample.flops()
        return flops


class PatchEmbedManual(nn.Module):
    r""" Image to Patch Embedding with Manual Conv2d

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
        weight (np.ndarray, optional): Pre-extracted conv weights
        bias (np.ndarray, optional): Pre-extracted conv bias
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, 
                 norm_layer=None, weight=None, bias=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        # Create ManualConv2d with the provided weights
        # ManualConv2d handles weight initialization internally
        self.proj = ManualConv2d(
            in_chans, 
            embed_dim, 
            kernel_size=patch_size, 
            stride=patch_size, 
            weight=weight, 
            bias=bias,
            bias_condition=True  # PatchEmbed always has bias
        )
        
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        
        # ManualConv2d returns a PyTorch tensor
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops


class ManualSwinTransformer(nn.Module):
    r""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030

    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False
        weights (dict, optional): Dictionary containing pre-extracted weights from PyTorch model
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, fused_window_process=False,
                 weights=None, **kwargs):
        super().__init__()

        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio

        # Extract weights if provided
        patch_embed_weights = None
        absolute_pos_embed_weights = None
        layer_weights_list = None
        norm_weights = None
        head_weights = None

        if weights is not None:
            patch_embed_weights = weights.get('patch_embed')
            absolute_pos_embed_weights = weights.get('absolute_pos_embed')
            layer_weights_list = weights.get('layers', [])
            norm_weights = weights.get('norm')
            head_weights = weights.get('head')

        # Split image into non-overlapping patches
        proj_weight = None
        proj_bias = None

        if patch_embed_weights is not None:
            proj_weight = patch_embed_weights.get('proj_weight')
            proj_bias = patch_embed_weights.get('proj_bias')

        self.patch_embed = PatchEmbedManual(
            img_size=img_size,
            patch_size=patch_size,
            in_chans=in_chans,
            embed_dim=embed_dim,
            norm_layer=LayerNorm if self.patch_norm else None,
            weight=proj_weight,
            bias=proj_bias
        )

        # If patch_norm is True and we have norm weights, set them
        if self.patch_norm and patch_embed_weights is not None:
            if 'norm_weight' in patch_embed_weights and patch_embed_weights['norm_weight'] is not None:
                self.patch_embed.norm.weight = patch_embed_weights['norm_weight']
            if 'norm_bias' in patch_embed_weights and patch_embed_weights['norm_bias'] is not None:
                self.patch_embed.norm.bias = patch_embed_weights['norm_bias']


        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # Absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            if absolute_pos_embed_weights is not None:
                with torch.no_grad():
                    self.absolute_pos_embed.copy_(torch.from_numpy(absolute_pos_embed_weights))
            else:
                trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # Stochastic depth
        dpr = [x.item() for x in torch.tensor(linspace_list(0, drop_path_rate, sum(depths)))]

        # Build layers
        self.layers = ModuleList()
        for i_layer in range(self.num_layers):
            # Get weights for this layer if available
            layer_weights = None
            if layer_weights_list is not None and i_layer < len(layer_weights_list):
                layer_weights = layer_weights_list[i_layer]

            # Extract blocks and downsample weights
            blocks_weights = None
            downsample_weights = None
            if layer_weights is not None:
                blocks_weights = layer_weights.get('blocks')
                downsample_weights = layer_weights.get('downsample')

            layer = ManualBasicLayer(
                dim=int(embed_dim * 2 ** i_layer),
                input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                 patches_resolution[1] // (2 ** i_layer)),
                depth=depths[i_layer],
                num_heads=num_heads[i_layer],
                window_size=window_size,
                mlp_ratio=self.mlp_ratio,
                qkv_bias=qkv_bias,
                norm_layer=norm_layer,
                downsample=ManualPatchMerging if (i_layer < self.num_layers - 1) else None,
                use_checkpoint=use_checkpoint,
                fused_window_process=fused_window_process,
                blocks_weights=blocks_weights,
                downsample_weights=downsample_weights
            )
            self.layers.append(layer)

        # Final norm
        self.norm = LayerNorm(normalized_shape=self.num_features)
        if norm_weights is not None:
            # For NumPy-based LayerNorm, directly assign NumPy arrays
            if 'weight' in norm_weights:
                self.norm.weight = norm_weights['weight']
            if 'bias' in norm_weights:
                self.norm.bias = norm_weights['bias']


        # Average pooling and classification head
        self.avgpool = AdaptiveAvgPool1d(1)
        self.head = ExplicitLinear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()

        if head_weights is not None and num_classes > 0:
            with torch.no_grad():
                if 'weight' in head_weights:
                    weight_tensor = torch.from_numpy(head_weights['weight'])
                    if isinstance(self.head.weight, nn.Parameter):
                        self.head.weight.data.copy_(weight_tensor)
                    else:
                         # If it's a NumPy array, replace it
                        self.head.weight = head_weights['weight']

                if 'bias' in head_weights and head_weights['bias'] is not None:
                    bias_tensor = torch.from_numpy(head_weights['bias'])
                    if isinstance(self.head.bias, nn.Parameter):
                        self.head.bias.data.copy_(bias_tensor)
                    else:
                         # If it's a NumPy array, replace it
                        self.head.bias = head_weights['bias']


        # Initialize weights if not provided
        if weights is None:
            self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, ExplicitLinear):
            # ExplicitLinear handles its own weight initialization if not provided
            pass
        elif isinstance(m, LayerNorm):
            # LayerNorm handles its own weight initialization if not provided
            pass
        elif isinstance(m, PatchEmbedManual):
            # PatchEmbedManual handles its own weight initialization if not provided
            pass
        elif isinstance(m, ManualSwinTransformerBlock):
             # ManualSwinTransformerBlock handles its own weight initialization if not provided
             pass
        elif isinstance(m, ManualBasicLayer):
             # ManualBasicLayer handles its own weight initialization if not provided
             pass
        elif isinstance(m, ManualPatchMerging):
             # ManualPatchMerging handles its own weight initialization if not provided
             pass
        elif isinstance(m, nn.Parameter):
             # Handle potential remaining nn.Parameters
             if m.numel() > 1: # Avoid single element parameters which might be biases handled elsewhere
                 trunc_normal_(m, std=.02)


    @torch.jit.ignore
    def no_weight_decay(self):
        return {'absolute_pos_embed'}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {'relative_position_bias_table'}

    def forward_features(self, x):
        print(f"--- forward_features start, input type: {type(x)}, shape: {x.shape}")
        # Ensure x is a tensor for consistency at the input
        if not isinstance(x, torch.Tensor):
            print("Converting input to torch tensor")
            x = torch.from_numpy(x).float()
        print(f"After input conversion, type: {type(x)}, shape: {x.shape}")

        # Patch embed returns a tensor
        print("Calling patch_embed...")
        x = self.patch_embed(x)
        print(f"After patch_embed, type: {type(x)}, shape: {x.shape}")


        # Ensure absolute_pos_embed is a tensor and on the same device/dtype
        if self.ape:
            print("Applying APE...")
            if not isinstance(self.absolute_pos_embed, torch.Tensor):
                 print("Converting APE to torch tensor")
                 self.absolute_pos_embed = torch.from_numpy(self.absolute_pos_embed).to(x.device, x.dtype)
            x = x + self.absolute_pos_embed
            print(f"After APE, type: {type(x)}, shape: {x.shape}")


        x = self.pos_drop(x)
        print(f"After pos_drop, type: {type(x)}, shape: {x.shape}")


        for i, layer in enumerate(self.layers):
            print(f"Calling layer {i}...")
            # ManualBasicLayer should handle tensor input and output tensor
            x = layer(x)
            print(f"After layer {i}, type: {type(x)}, shape: {x.shape}")


        # The norm should handle tensors properly with your implementation
        # Assuming LayerNorm handles both numpy and torch
        print("Calling final norm...")
        x = self.norm(x)  # B L C
        print(f"After final norm, type: {type(x)}, shape: {x.shape}")


        # Ensure we're working with tensors for the remaining operations
        if not isinstance(x, torch.Tensor):
            print("Converting final norm output to torch tensor")
            x = torch.from_numpy(x).float()
        print(f"Before transpose, type: {type(x)}, shape: {x.shape}")


        x = x.transpose(1, 2)  # B C L
        print(f"After transpose, type: {type(x)}, shape: {x.shape}")

        x = self.avgpool(x)  # B C 1
        print(f"After avgpool, type: {type(x)}, shape: {x.shape}")

        x = torch.flatten(x, 1)  # B C
        print(f"After flatten, type: {type(x)}, shape: {x.shape}")
        print("--- forward_features end ---")
        return x

    def forward(self, x):
        print(f"--- Full forward start, input type: {type(x)}, shape: {x.shape}")
        x = self.forward_features(x)
        print(f"After forward_features, type: {type(x)}, shape: {x.shape}")


        # Handle classification head which expects NumPy input
        if not isinstance(self.head, nn.Identity):
            print("Calling classification head...")
            # Ensure x is a NumPy array for ExplicitLinear
            if isinstance(x, torch.Tensor):
                print("Converting forward_features output to numpy for head")
                device = x.device
                dtype = x.dtype
                x_np = x.detach().cpu().numpy()
                x_out_np = self.head(x_np)
                # Convert back to tensor to match PyTorch implementation
                print("Converting head output back to torch tensor")
                x = torch.from_numpy(x_out_np).to(device=device, dtype=dtype)
            else:
                print("Input to head is already numpy")
                # If input was already numpy, head returns numpy
                x = self.head(x)
                # Convert to tensor before returning to match PyTorch output type
                print("Converting head output to torch tensor")
                x = torch.from_numpy(x).float()
        else:
            print("Head is nn.Identity, passing through")
            x = self.head(x) # Identity returns the input as is (tensor)

        print(f"--- Full forward end, output type: {type(x)}, shape: {x.shape}")
        return x


    def flops(self):
        flops = 0
        flops += self.patch_embed.flops()
        for i, layer in enumerate(self.layers):
            flops += layer.flops()
        # Flops for final norm, avgpool, and head
        # The provided flops calculation for SwinTransformer seems to have a slight discrepancy
        # Let's replicate the original calculation's structure
        Ho, Wo = self.patches_resolution
        num_patches = Ho * Wo
        # Flops for final norm
        flops += self.num_features * num_patches // (2 ** (self.num_layers -1)) # This part seems off in original
        # Replicating original structure:
        final_resolution = (self.patches_resolution[0] // (2 ** (self.num_layers -1)),
                            self.patches_resolution[1] // (2 ** (self.num_layers -1)))
        final_num_patches = final_resolution[0] * final_resolution[1]
        flops += self.num_features * final_num_patches # Norm after the last layer

        # Avgpool: B x C x 1, effectively a reduction along L dimension (size final_num_patches)
        # Flops are typically considered negligible or zero for pooling
        # If we consider it as sum/count: B * C * final_num_patches

        # Head: B x C -> B x num_classes
        flops += self.num_features * self.num_classes

        return flops